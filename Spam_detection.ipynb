{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification\n",
    "\n",
    "This project I will be looking into SMS text data from multiple sources all collected by the team [Tiago A. Almeida](http://dcomp.sor.ufscar.br/talmeida/) and [José María Gómez Hidalgo](http://www.esp.uem.es/jmgomez). For more information on how they collected this data check it out [here](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).\n",
    "\n",
    "Some notable sources used while performing this analysis and classification: \n",
    "- [Ultimate guide to deal with Text Data (using Python)](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
    "\n",
    "The data here is a collection of 747 Spam texts along with 4,827 non-spam (HAM) texts. The file is formatted as a plain text file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the file. \n",
    "\n",
    "from exploring the data we know that we need to strip the new line characters (__\\n__) and that the message and label are separated by a tab (__\\t__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/SMSSpamCollection.txt') as f:\n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "sms_df.rename(columns={0:'label', 1:'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Engineering\n",
    "\n",
    "Before any of this I did a little bit of text exploring to see if I could see anything that may or may not help me--this was crucial for choosing __'flag words'__. Items like word count, char counts, number of numerics, number of upper case, etc. are pretty common practice, so they are great features to add to your data set before cleaning. \n",
    "\n",
    "1. word count\n",
    "2. character count\n",
    "3. Number of numerics\n",
    "4. Number of upper case\n",
    "5. Number of Exclamation Points (!)\n",
    "6. Number of Flag Words\n",
    "7. Links in message\n",
    "8. Count of stop words\n",
    "\n",
    "\n",
    "__ 1. word count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['word_count'] = sms_df.text.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. character count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['char_count'] = sms_df.text.str.len() #this includes the spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Number of numerics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['numerics'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Number of upper case characters__\n",
    "\n",
    "this returns how many words in the message are all-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['upper'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Number of Excalmation Points (!)__\n",
    "\n",
    "This splits the message a returns how many times the message has been split minus 1. This will return the total number of '!' in the message. e.g. if we have a message: 'Hey!' it will return ['Hey',''], so we subtract one to get # of excalamtion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['bangs'] = sms_df.text.apply(lambda x: len([x for x in x.split('!')]) - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Flag Words__\n",
    "\n",
    "Shout-out to [Grace](https://github.com/graceh3) for this idea! \n",
    "\n",
    "Possible __\"flag\"__ words from looking at the first few rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['flag_words'] = sms_df.text.apply(lambda x: len([x for x in x.split(' ') \n",
    "                                                        if x.translate(str.maketrans('', '', string.punctuation)).lower().strip() \n",
    "                                                        in ['winner','urgent','win','won','free','cash','freemsg',\n",
    "                                                            'stopsms','ppm']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Links in message__\n",
    "\n",
    "We will use regex to be able to see if there are any links in the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "sms_df['links'] = sms_df.text.apply(lambda x: len(re.findall(p, x))) + sms_df.text.apply(lambda x: x.count('.com')+x.count('.co')+x.count('.uk'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.links.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ham</td>\n",
       "      <td>I only haf msn. It's yijue@hotmail.com</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>spam</td>\n",
       "      <td>Are you unique enough? Find out from 30th Augu...</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>spam</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>23</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ur ringtone service has changed! 25 Free credi...</td>\n",
       "      <td>27</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text  word_count  \\\n",
       "15   spam  XXXMobileMovieClub: To use your credit, click ...          19   \n",
       "136   ham             I only haf msn. It's yijue@hotmail.com           6   \n",
       "191  spam  Are you unique enough? Find out from 30th Augu...          10   \n",
       "250  spam  Congratulations ur awarded 500 of CD vouchers ...          23   \n",
       "268  spam  Ur ringtone service has changed! 25 Free credi...          27   \n",
       "\n",
       "     char_count  numerics  upper  bangs  flag_words  links  \n",
       "15          149         0      1      0           0      3  \n",
       "136          38         0      1      0           0      2  \n",
       "191          72         0      0      0           0      2  \n",
       "250         150         4      2      0           1      2  \n",
       "268         159         1      5      3           1      2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df[sms_df['links'] >= 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Count of Stop Words__\n",
    "\n",
    "these are words that don't add any real value to our messages, which include: of, the, on, etc. Without them our sentences would not be great, but they don't impact meaning in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "sms_df['stp_wrd_cnt'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lets look at the first few columns to see how all these new columns look__\n",
    "\n",
    "So far, these engineered columns are looking _great!_ They should have a big impact on our spam predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "      <th>stp_wrd_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  word_count  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          20   \n",
       "1   ham                      Ok lar... Joking wif u oni...           6   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          28   \n",
       "3   ham  U dun say so early hor... U c already then say...          11   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          13   \n",
       "\n",
       "   char_count  numerics  upper  bangs  flag_words  links  stp_wrd_cnt  \n",
       "0         111         0      0      0           0      0            4  \n",
       "1          29         0      0      0           0      0            0  \n",
       "2         155         2      2      0           2      0            5  \n",
       "3          49         0      2      0           0      0            2  \n",
       "4          61         0      1      0           0      0            5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, we need to move into data cleaning. This section will be very important for the remaineder of this project and the models we run. In the next few cells we will:\n",
    "1. create a function to remove all punction\n",
    "2. lower case all of the words in our messages\n",
    "3. remove stop words\n",
    "4. check for spelling and correct where needed\n",
    "5. remove frequent\n",
    "6. remove rare/uncommon words\n",
    "\n",
    "\n",
    "#### 1) and 2) get rid of special charaters and lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_column(row):\n",
    "    import string\n",
    "    '''\n",
    "    takes in a cell from the dataframe and removes all of the symbols from \n",
    "    string.punctuation ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'), and then lower\n",
    "    cases each line.\n",
    "    '''\n",
    "    return row.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda row: clean_text_column(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what our function did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s\n",
      "\n",
      "label                                                       spam\n",
      "text           free entry in 2 a wkly comp to win fa cup fina...\n",
      "word_count                                                    28\n",
      "char_count                                                   155\n",
      "numerics                                                       2\n",
      "upper                                                          2\n",
      "bangs                                                          0\n",
      "flag_words                                                     2\n",
      "links                                                          0\n",
      "stp_wrd_cnt                                                    5\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sms_df.text.iloc[2],sms_df.iloc[2],sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove all stop words:\n",
    "\n",
    "Here we will remove all of the words that do not add value to the meaning of our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #loads the stop words for the english language\n",
    "sms_df.text = sms_df.text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n",
    "#returns only words that are not in the list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correct Obvious Spelling issues:\n",
    "\n",
    "First, lets fix the most commonly 'missspelled' words. After, we will use textBlob to correct our spelling for all the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u         1132\n",
       "call       578\n",
       "2          482\n",
       "im         464\n",
       "ur         390\n",
       "get        386\n",
       "4          293\n",
       "dont       287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "got        239\n",
       "ill        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "text       188\n",
       "one        171\n",
       "going      171\n",
       "ü          169\n",
       "need       167\n",
       "txt        163\n",
       "home       162\n",
       "lor        160\n",
       "see        157\n",
       "sorry      156\n",
       "still      154\n",
       "r          153\n",
       "stop       152\n",
       "back       152\n",
       "n          146\n",
       "reply      144\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "later      134\n",
       "well       134\n",
       "hi         133\n",
       "think      132\n",
       "da         131\n",
       "please     129\n",
       "take       126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of these are abbreviations, so lets change these in our dataset before we remove frequent and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = {' wif u ':' with you ', ' u r ':' you are ', 'ic': ' i see ', ' i c ': ' i see ',\n",
    "       ' u c ': ' you see ',' u ':' you ',' n ':' and ',' r ':' are ',' txt ':' text ',\n",
    "       ' ü ':' you ',' 4 ':' for ',' 2 ':' to ',' ur ':' your ',' da ':' the ',' wif ':' with ',\n",
    "       ' urself ':' yourself ', ' thats ': ' thats ', ' i‘m ': ' im ', '£':''}\n",
    "\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "sms_df.text = sms_df.text.apply(lambda x: pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip())\n",
    "# text = pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now if we look at the most common words, we can see the changes that we have made__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you       1203\n",
       "see        961\n",
       "i          794\n",
       "call       578\n",
       "im         469\n",
       "to         464\n",
       "get        386\n",
       "your       349\n",
       "text       346\n",
       "e          320\n",
       "for        287\n",
       "dont       287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "ill        239\n",
       "got        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "k          207\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "one        171\n",
       "going      171\n",
       "need       167\n",
       "home       162\n",
       "lor        160\n",
       "sorry      157\n",
       "still      154\n",
       "back       152\n",
       "stop       152\n",
       "are        149\n",
       "p          146\n",
       "reply      144\n",
       "and        141\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "later      134\n",
       "well       134\n",
       "hi         133\n",
       "think      132\n",
       "please     129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Remove Very Frequent Words:\n",
    "Super common words do not add value to their connection with other words, which is why we will remove the top ten most common words in the cell below. Becuase this is text data, I decided it would be fine in this case to remove these words. [Here](https://www.quora.com/Why-do-we-remove-frequent-and-infrequent-words-when-in-NLP) is a great explaination for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:15]\n",
    "#Pretty good sign that most of these seem like filler words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now that we've checked on them, lets remove them from our texts\n",
    "sms_df.text = sms_df.text.apply(lambda x: ' '.join(x for x in x.split() if x not in list(common_w.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Use TextBlob to correct spelling: \n",
    "\n",
    "Now that we've removed a good portion of words that add noise to our data, we should clean up the words in one final step to correct spelling. \n",
    "\n",
    "There are many libraries to do this, but for convenience sake I have stuck with this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.to_csv('Data/spelling_and_features_sms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words in each message\n",
    "\n",
    "below we will use the nltk word tokenizer to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_message(row):\n",
    "    return word_tokenize(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda row: tokenize_message(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_words(row,li):\n",
    "    li.append(' '.join(set(row)))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "sms_df.text.apply(lambda row: get_all_words(row,list_of_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list_of_words = ' '.join(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list_of_words = new_list_of_words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(new_list_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing:\n",
    "\n",
    "blah blah blah \n",
    "Great article [here](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python) about lemmatizing and stemming with NLTK.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str=”been had done languages cities mice”\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
