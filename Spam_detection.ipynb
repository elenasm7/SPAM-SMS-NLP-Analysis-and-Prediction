{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification\n",
    "\n",
    "This project I will be looking into SMS text data from multiple sources all collected by the team [Tiago A. Almeida](http://dcomp.sor.ufscar.br/talmeida/) and [José María Gómez Hidalgo](http://www.esp.uem.es/jmgomez). For more information on how they collected this data check it out [here](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).\n",
    "\n",
    "Some notable sources used while performing this analysis and classification: \n",
    "- [Ultimate guide to deal with Text Data (using Python)](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
    "\n",
    "The data here is a collection of 747 Spam texts along with 4,827 non-spam (HAM) texts. The file is formatted as a plain text file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the file. \n",
    "\n",
    "from exploring the data we know that we need to strip the new line characters (__\\n__) and that the message and label are separated by a tab (__\\t__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/SMSSpamCollection.txt') as f:\n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "sms_df.rename(columns={0:'label', 1:'text'},inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "sms_df['target'] = le.fit_transform(sms_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  target\n",
       "0   ham  Go until jurong point, crazy.. Available only ...       0\n",
       "1   ham                      Ok lar... Joking wif u oni...       0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Engineering\n",
    "\n",
    "Before any of this I did a little bit of text exploring to see if I could see anything that may or may not help me--this was crucial for choosing __'flag words'__. Items like word count, char counts, number of numerics, number of upper case, etc. are pretty common practice, so they are great features to add to your data set before cleaning. \n",
    "\n",
    "1. word count\n",
    "2. character count\n",
    "3. Number of numerics\n",
    "4. Number of upper case\n",
    "5. Number of Exclamation Points (!)\n",
    "6. Number of Flag Words\n",
    "7. Links in message\n",
    "8. Count of stop words\n",
    "\n",
    "\n",
    "__ 1. word count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['word_count'] = sms_df.text.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. character count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['char_count'] = sms_df.text.str.len() #this includes the spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Number of numerics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['numerics'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Number of upper case characters__\n",
    "\n",
    "this returns how many words in the message are all-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['upper'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Number of Excalmation Points (!)__\n",
    "\n",
    "This splits the message a returns how many times the message has been split minus 1. This will return the total number of '!' in the message. e.g. if we have a message: 'Hey!' it will return ['Hey',''], so we subtract one to get # of excalamtion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['bangs'] = sms_df.text.apply(lambda x: len([x for x in x.split('!')]) - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Flag Words__\n",
    "\n",
    "Shout-out to [Grace](https://github.com/graceh3) for this idea! \n",
    "\n",
    "Possible __\"flag\"__ words from looking at the first few rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['flag_words'] = sms_df.text.apply(lambda x: len([x for x in x.split(' ') \n",
    "                                                        if x.translate(str.maketrans('', '', string.punctuation)).lower().strip() \n",
    "                                                        in ['winner','urgent','win','won','free','cash','freemsg',\n",
    "                                                            'stopsms','ppm']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Links in message__\n",
    "\n",
    "We will use regex to be able to see if there are any links in the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "sms_df['links'] = sms_df.text.apply(lambda x: len(re.findall(p, x))) + sms_df.text.apply(lambda x: x.count('.com')+x.count('.co')+x.count('.uk'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.links.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ham</td>\n",
       "      <td>I only haf msn. It's yijue@hotmail.com</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>spam</td>\n",
       "      <td>Are you unique enough? Find out from 30th Augu...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>spam</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ur ringtone service has changed! 25 Free credi...</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text  target  \\\n",
       "15   spam  XXXMobileMovieClub: To use your credit, click ...       1   \n",
       "136   ham             I only haf msn. It's yijue@hotmail.com       0   \n",
       "191  spam  Are you unique enough? Find out from 30th Augu...       1   \n",
       "250  spam  Congratulations ur awarded 500 of CD vouchers ...       1   \n",
       "268  spam  Ur ringtone service has changed! 25 Free credi...       1   \n",
       "\n",
       "     word_count  char_count  numerics  upper  bangs  flag_words  links  \n",
       "15           19         149         0      1      0           0      3  \n",
       "136           6          38         0      1      0           0      2  \n",
       "191          10          72         0      0      0           0      2  \n",
       "250          23         150         4      2      0           1      2  \n",
       "268          27         159         1      5      3           1      2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df[sms_df['links'] >= 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Count of Stop Words__\n",
    "\n",
    "these are words that don't add any real value to our messages, which include: of, the, on, etc. Without them our sentences would not be great, but they don't impact meaning in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "sms_df['stp_wrd_cnt'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lets look at the first few columns to see how all these new columns look__\n",
    "\n",
    "So far, these engineered columns are looking _great!_ They should have a big impact on our spam predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "      <th>stp_wrd_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  target  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...       0   \n",
       "1   ham                      Ok lar... Joking wif u oni...       0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...       1   \n",
       "3   ham  U dun say so early hor... U c already then say...       0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...       0   \n",
       "\n",
       "   word_count  char_count  numerics  upper  bangs  flag_words  links  \\\n",
       "0          20         111         0      0      0           0      0   \n",
       "1           6          29         0      0      0           0      0   \n",
       "2          28         155         2      2      0           2      0   \n",
       "3          11          49         0      2      0           0      0   \n",
       "4          13          61         0      1      0           0      0   \n",
       "\n",
       "   stp_wrd_cnt  \n",
       "0            4  \n",
       "1            0  \n",
       "2            5  \n",
       "3            2  \n",
       "4            5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, we need to move into data cleaning. This section will be very important for the remaineder of this project and the models we run. In the next few cells we will:\n",
    "1. create a function to remove all punction\n",
    "2. lower case all of the words in our messages\n",
    "3. remove stop words\n",
    "4. check for spelling and correct where needed\n",
    "5. remove frequent\n",
    "6. remove rare/uncommon words\n",
    "\n",
    "\n",
    "#### 1) and 2) get rid of special charaters and lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_column(row):\n",
    "    import string\n",
    "    '''\n",
    "    takes in a cell from the dataframe and removes all of the symbols from \n",
    "    string.punctuation ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'), and then lower\n",
    "    cases each line.\n",
    "    '''\n",
    "    return row.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda row: clean_text_column(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what our function did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s\n",
      "\n",
      "label                                                       spam\n",
      "text           free entry in 2 a wkly comp to win fa cup fina...\n",
      "target                                                         1\n",
      "word_count                                                    28\n",
      "char_count                                                   155\n",
      "numerics                                                       2\n",
      "upper                                                          2\n",
      "bangs                                                          0\n",
      "flag_words                                                     2\n",
      "links                                                          0\n",
      "stp_wrd_cnt                                                    5\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sms_df.text.iloc[2],sms_df.iloc[2],sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove all stop words:\n",
    "\n",
    "Here we will remove all of the words that do not add value to the meaning of our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #loads the stop words for the english language\n",
    "sms_df.text = sms_df.text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n",
    "#returns only words that are not in the list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correct Obvious Spelling issues:\n",
    "\n",
    "First, lets fix the most commonly 'missspelled' words. After, we will use textBlob to correct our spelling for all the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u         1132\n",
       "call       578\n",
       "2          482\n",
       "im         464\n",
       "ur         390\n",
       "get        386\n",
       "4          293\n",
       "dont       287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "ill        239\n",
       "got        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "text       188\n",
       "going      171\n",
       "one        171\n",
       "ü          169\n",
       "need       167\n",
       "txt        163\n",
       "home       162\n",
       "lor        160\n",
       "see        157\n",
       "sorry      156\n",
       "still      154\n",
       "r          153\n",
       "stop       152\n",
       "back       152\n",
       "n          146\n",
       "reply      144\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "well       134\n",
       "later      134\n",
       "hi         133\n",
       "think      132\n",
       "da         131\n",
       "please     129\n",
       "take       126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of these are abbreviations, so lets change these in our dataset before we remove frequent and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep = {' wif u ':' with you ', ' u r ':' you are ', 'ic': ' i see ', ' i c ': ' i see ',\n",
    "       ' u c ': ' you see ',' u ':' you ',' n ':' and ',' r ':' are ',' txt ':' text ',\n",
    "       ' ü ':' you ',' 4 ':' for ',' 2 ':' to ',' ur ':' your ',' da ':' the ',' wif ':' with ',\n",
    "       ' urself ':' yourself ', ' thats ': ' thats ', ' i‘m ': ' im ', '£':''}\n",
    "\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "sms_df.text = sms_df.text.apply(lambda x: pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip())\n",
    "# text = pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now if we look at the most common words, we can see the changes that we have made__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you       1203\n",
       "see        961\n",
       "i          794\n",
       "call       578\n",
       "im         469\n",
       "to         464\n",
       "get        386\n",
       "your       349\n",
       "text       346\n",
       "e          320\n",
       "dont       287\n",
       "for        287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "got        239\n",
       "ill        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "k          207\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "one        171\n",
       "going      171\n",
       "need       167\n",
       "home       162\n",
       "lor        160\n",
       "sorry      157\n",
       "still      154\n",
       "back       152\n",
       "stop       152\n",
       "are        149\n",
       "p          146\n",
       "reply      144\n",
       "and        141\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "well       134\n",
       "later      134\n",
       "hi         133\n",
       "think      132\n",
       "please     129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Remove Very Frequent Words:\n",
    "Super common words do not add value to their connection with other words, which is why we will remove the top ten most common words in the cell below. Becuase this is text data, I decided it would be fine in this case to remove these words. [Here](https://www.quora.com/Why-do-we-remove-frequent-and-infrequent-words-when-in-NLP) is a great explaination for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:15]\n",
    "#Pretty good sign that most of these seem like filler words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now that we've checked on them, lets remove them from our texts\n",
    "sms_df.text = sms_df.text.apply(lambda x: ' '.join(x for x in x.split() if x not in list(common_w.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Use SymSpell to correct spelling: \n",
    "\n",
    "Now that we've removed a good portion of words that add noise to our data, we should clean up the words in one final step to correct spelling. \n",
    "\n",
    "There are many libraries to do this, but this one is very quick and compared to others I've used appears to be very accurate. We will correct the spelling of words that occur once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy.symspellpy import SymSpell\n",
    "sym_spell = SymSpell(2, 7)\n",
    "sym_spell.load_dictionary('frequency_dictionary_en_82_765.txt', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of words that only occur once:\n",
    "word_lists = list(sms_df.text.apply(lambda x: x.split(' ')))\n",
    "all_words = [word for rev in word_lists for word in rev]\n",
    "corpus_word_counts_df = pd.DataFrame(pd.Series(all_words).value_counts()).reset_index()\\\n",
    ".rename(columns={'index':'words',0:'counts'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_word_counts_df_1 = corpus_word_counts_df[corpus_word_counts_df['counts'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91139969d9434c1c921efc4605243632"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/elenasm7/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "\n",
    "tqdm_notebook.pandas(desc=\"Progress: \")\n",
    "\n",
    "corpus_word_counts_df_1['corrected'] = corpus_word_counts_df_1.words.progress_apply(lambda w: \n",
    "                                                                                     sym_spell.word_segmentation(w)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace words that were corrected in this last step by applying the function we're defining below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_fixed_words(message,df):\n",
    "    words = message.split(' ')\n",
    "    cor_rev = []\n",
    "    for word in words: \n",
    "        if word in list(df.words):\n",
    "            cor_rev.append(df[df['words'] == word]['corrected'].item())\n",
    "        else:\n",
    "            cor_rev.append(word)\n",
    "    return ' '.join(cor_rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ba57b6c1f64751a6e945fdbefb3965"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sms_df['text_corr'] = sms_df\\\n",
    ".text.progress_apply(lambda x: replace_fixed_words(x,corpus_word_counts_df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.to_csv('Data/spelling_and_features_sms.csv')\n",
    "# sms_df = pd.read_csv('Data/spelling_and_features_sms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing:\n",
    "\n",
    "I originally planned on using NLTK amd the part of speech to lemmatize. However, I later discovered Spacy, which is very efficient and accurate. They have amazing [documentation](https://spacy.io/api/annotation#lemmatization) for lemmatizing and everything else the librazry offers, which is _a lot_. \n",
    "\n",
    "__Here is a little information on what I previously planned on doing, along with some great articles to help:__\n",
    "\n",
    "Great article [here](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python) about lemmatizing and stemming with NLTK.\n",
    "\n",
    "If you get an error about POS (part of speech) tagging, then you may need to download treebank and punkt from NLTK. [Here](https://stackoverflow.com/questions/14089887/nltk-pos-tag-usage) is the resource I used. However, mine did not work until I downloaded 'averaged_perceptron_tagger'.\n",
    "\n",
    "Below I defined a function that takes the returned pos values from nltk.pos_tag, and returns a compatible version of them. This was a great solution on [stackoverflow](https://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def return_lemma(review,nlp):\n",
    "    doc = nlp(review)\n",
    "    return ' '.join([word.lemma_ for word in doc])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sms_df['lemmed']=sms_df.text.apply(lambda txt: return_lemma(txt,nlp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    jurong point crazy available bugi and great wo...\n",
       "1                                    lar joke with oni\n",
       "2    free entry wkly comp win fa cup final tkts 21s...\n",
       "3                        dun say early hor already say\n",
       "4                  nah think go usf life around though\n",
       "5    freemsg hey darle 3 week word back -PRON- woul...\n",
       "6        even brother like speak treat like aid patent\n",
       "7    per request melle melle oru minnaminunginte nu...\n",
       "8    winner value network customer select receivea ...\n",
       "9    mobile 11 month be entitle update late colour ...\n",
       "Name: lemmed, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.lemmed[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words in each message\n",
    "\n",
    "below we will use the nltk word tokenizer to accomplish this. This function will train-test split our data. We are setting a random state of 40. I have also chosen to do a 80/20 split based on the pareto principle.\n",
    "\n",
    "First, let's split up our columns into predictors (X) and Targets. For this we will have two separate sets of predictors. X1 will be just the corrected text and X2 will be everything else except for texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text', 'target', 'word_count', 'char_count', 'numerics',\n",
       "       'upper', 'bangs', 'flag_words', 'links', 'stp_wrd_cnt', 'text_corr',\n",
       "       'lemmed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1 = sms_df.lemmed\n",
    "X2 = sms_df.iloc[:,3:11]\n",
    "y = sms_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Let us train-test split the non-text predictors using the same parameters (test_size and random_state)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X2, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(X, y): \n",
    "    '''\n",
    "    Generate train and test TF-IDF vectorization for our data set\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: pandas.Series object\n",
    "        Pandas series of text documents to classify \n",
    "    y : pandas.Series object\n",
    "        Pandas series containing label for each document\n",
    "    Returns\n",
    "    --------\n",
    "    tf_idf_train :  sparse matrix, [n_train_samples, n_features]\n",
    "        Vector representation of train data\n",
    "    tf_idf_test :  sparse matrix, [n_test_samples, n_features]\n",
    "        Vector representation of test data\n",
    "    y_train : array-like object\n",
    "        labels for training data\n",
    "    y_test : array-like object\n",
    "        labels for testing data\n",
    "    vectorizer : vectorizer object\n",
    "        fit TF-IDF vecotrizer object\n",
    "\n",
    "    '''\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "    tf_idf_train = vectorizer.fit_transform(X_train)\n",
    "    tf_idf_test = vectorizer.transform(X_test)\n",
    "    return tf_idf_train,tf_idf_test, y_train, y_test, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_train,tf_idf_test, y_train, y_test, vectorizer = tfidf(X1, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions: \n",
    "\n",
    "Next, I will use muliple models to predict if the message is spam or not. After I will take the average of the results and make the final prediction.\n",
    "\n",
    "__First lets do it just for the tf-idf matrix:__\n",
    "\n",
    "I have defined a function called \"classify_text\", you pass it the classifier, tf_idf_train, tf_idf_test, and y_train and it will return the predictions for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_text(classifier, tf_idf_train, tf_idf_test, y_train):\n",
    "    '''\n",
    "    Train a classifier to identify whether a message is spam or ham\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    classifier: sklearn classifier\n",
    "       initialized sklearn classifier (MultinomialNB, RandomForestClassifier, etc.)\n",
    "    tf_idf_train : sparse matrix, [n_train_samples, n_features]\n",
    "        TF-IDF vectorization of train data\n",
    "    tf_idf_test : sparse matrix, [n_test_samples, n_features]\n",
    "        TF-IDF vectorization of test data\n",
    "    y_train : pandas.Series object\n",
    "        Pandas series containing label for each document in the train set\n",
    "    Returns\n",
    "    --------\n",
    "    train_preds :  list object\n",
    "        Predictions for train data\n",
    "    test_preds :  list object\n",
    "        Predictions for test data\n",
    "    '''\n",
    "    clf = classifier\n",
    "    clf.fit(tf_idf_train, y_train)\n",
    "    return clf.predict(tf_idf_train), clf.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initalize a random forest classifier and then pass the sbove function the correct objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(n_estimators=100)\n",
    "rf_train_preds, rf_test_preds = classify_text(rf_classifier,tf_idf_train, tf_idf_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[953   1]\n",
      " [ 30 131]]\n",
      "0.9721973094170404\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, rf_test_preds))\n",
    "print(accuracy_score(y_test, rf_test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our random forest classifier is very accurate. We could definitly just stop here, however lets try to make it even better!\n",
    "\n",
    "Now, I will do a model for our other predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
