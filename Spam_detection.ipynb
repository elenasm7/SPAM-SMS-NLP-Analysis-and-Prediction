{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMS Spam Classification\n",
    "\n",
    "This project I will be looking into SMS text data from multiple sources all collected by the team [Tiago A. Almeida](http://dcomp.sor.ufscar.br/talmeida/) and [José María Gómez Hidalgo](http://www.esp.uem.es/jmgomez). For more information on how they collected this data check it out [here](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/).\n",
    "\n",
    "Some notable sources used while performing this analysis and classification: \n",
    "- [Ultimate guide to deal with Text Data (using Python)](https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/)\n",
    "\n",
    "The data here is a collection of 747 Spam texts along with 4,827 non-spam (HAM) texts. The file is formatted as a plain text file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "np.random.seed(0)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the file. \n",
    "\n",
    "from exploring the data we know that we need to strip the new line characters (__\\n__) and that the message and label are separated by a tab (__\\t__)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('Data/SMSSpamCollection.txt') as f:\n",
    "    lines = [line.rstrip('\\n').split('\\t') for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df = pd.DataFrame(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5574, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()\n",
    "sms_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#rename the columns\n",
    "sms_df.rename(columns={0:'label', 1:'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Feature Engineering\n",
    "\n",
    "Before any of this I did a little bit of text exploring to see if I could see anything that may or may not help me--this was crucial for choosing __'flag words'__. Items like word count, char counts, number of numerics, number of upper case, etc. are pretty common practice, so they are great features to add to your data set before cleaning. \n",
    "\n",
    "1. word count\n",
    "2. character count\n",
    "3. Number of numerics\n",
    "4. Number of upper case\n",
    "5. Number of Exclamation Points (!)\n",
    "6. Number of Flag Words\n",
    "7. Links in message\n",
    "8. Count of stop words\n",
    "\n",
    "\n",
    "__ 1. word count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['word_count'] = sms_df.text.apply(lambda x: len(str(x).split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. character count__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['char_count'] = sms_df.text.str.len() #this includes the spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3. Number of numerics__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['numerics'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4. Number of upper case characters__\n",
    "\n",
    "this returns how many words in the message are all-caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['upper'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x.isupper()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5. Number of Excalmation Points (!)__\n",
    "\n",
    "This splits the message a returns how many times the message has been split minus 1. This will return the total number of '!' in the message. e.g. if we have a message: 'Hey!' it will return ['Hey',''], so we subtract one to get # of excalamtion points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['bangs'] = sms_df.text.apply(lambda x: len([x for x in x.split('!')]) - 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6. Flag Words__\n",
    "\n",
    "Shout-out to [Grace](https://github.com/graceh3) for this idea! \n",
    "\n",
    "Possible __\"flag\"__ words from looking at the first few rows of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df['flag_words'] = sms_df.text.apply(lambda x: len([x for x in x.split(' ') \n",
    "                                                        if x.translate(str.maketrans('', '', string.punctuation)).lower().strip() \n",
    "                                                        in ['winner','urgent','win','won','free','cash','freemsg',\n",
    "                                                            'stopsms','ppm']]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__7. Links in message__\n",
    "\n",
    "We will use regex to be able to see if there are any links in the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "sms_df['links'] = sms_df.text.apply(lambda x: len(re.findall(p, x))) + sms_df.text.apply(lambda x: x.count('.com')+x.count('.co')+x.count('.uk'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.links.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spam</td>\n",
       "      <td>XXXMobileMovieClub: To use your credit, click ...</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>ham</td>\n",
       "      <td>I only haf msn. It's yijue@hotmail.com</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>spam</td>\n",
       "      <td>Are you unique enough? Find out from 30th Augu...</td>\n",
       "      <td>10</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>spam</td>\n",
       "      <td>Congratulations ur awarded 500 of CD vouchers ...</td>\n",
       "      <td>23</td>\n",
       "      <td>150</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>spam</td>\n",
       "      <td>Ur ringtone service has changed! 25 Free credi...</td>\n",
       "      <td>27</td>\n",
       "      <td>159</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                               text  word_count  \\\n",
       "15   spam  XXXMobileMovieClub: To use your credit, click ...          19   \n",
       "136   ham             I only haf msn. It's yijue@hotmail.com           6   \n",
       "191  spam  Are you unique enough? Find out from 30th Augu...          10   \n",
       "250  spam  Congratulations ur awarded 500 of CD vouchers ...          23   \n",
       "268  spam  Ur ringtone service has changed! 25 Free credi...          27   \n",
       "\n",
       "     char_count  numerics  upper  bangs  flag_words  links  \n",
       "15          149         0      1      0           0      3  \n",
       "136          38         0      1      0           0      2  \n",
       "191          72         0      0      0           0      2  \n",
       "250         150         4      2      0           1      2  \n",
       "268         159         1      5      3           1      2  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df[sms_df['links'] >= 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__8. Count of Stop Words__\n",
    "\n",
    "these are words that don't add any real value to our messages, which include: of, the, on, etc. Without them our sentences would not be great, but they don't impact meaning in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "sms_df['stp_wrd_cnt'] = sms_df.text.apply(lambda x: len([x for x in x.split() if x in stop]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Lets look at the first few columns to see how all these new columns look__\n",
    "\n",
    "So far, these engineered columns are looking _great!_ They should have a big impact on our spam predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>numerics</th>\n",
       "      <th>upper</th>\n",
       "      <th>bangs</th>\n",
       "      <th>flag_words</th>\n",
       "      <th>links</th>\n",
       "      <th>stp_wrd_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>20</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>28</td>\n",
       "      <td>155</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text  word_count  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          20   \n",
       "1   ham                      Ok lar... Joking wif u oni...           6   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          28   \n",
       "3   ham  U dun say so early hor... U c already then say...          11   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          13   \n",
       "\n",
       "   char_count  numerics  upper  bangs  flag_words  links  stp_wrd_cnt  \n",
       "0         111         0      0      0           0      0            4  \n",
       "1          29         0      0      0           0      0            0  \n",
       "2         155         2      2      0           2      0            5  \n",
       "3          49         0      2      0           0      0            2  \n",
       "4          61         0      1      0           0      0            5  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next, we need to move into data cleaning. This section will be very important for the remaineder of this project and the models we run. In the next few cells we will:\n",
    "1. create a function to remove all punction\n",
    "2. lower case all of the words in our messages\n",
    "3. remove stop words\n",
    "4. check for spelling and correct where needed\n",
    "5. remove frequent\n",
    "6. remove rare/uncommon words\n",
    "\n",
    "\n",
    "#### 1) and 2) get rid of special charaters and lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text_column(row):\n",
    "    import string\n",
    "    '''\n",
    "    takes in a cell from the dataframe and removes all of the symbols from \n",
    "    string.punctuation ('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'), and then lower\n",
    "    cases each line.\n",
    "    '''\n",
    "    return row.translate(str.maketrans('', '', string.punctuation)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda row: clean_text_column(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what our function did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s\n",
      "\n",
      "label                                                       spam\n",
      "text           free entry in 2 a wkly comp to win fa cup fina...\n",
      "word_count                                                    28\n",
      "char_count                                                   155\n",
      "numerics                                                       2\n",
      "upper                                                          2\n",
      "bangs                                                          0\n",
      "flag_words                                                     2\n",
      "links                                                          0\n",
      "stp_wrd_cnt                                                    5\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(sms_df.text.iloc[2],sms_df.iloc[2],sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove all stop words:\n",
    "\n",
    "Here we will remove all of the words that do not add value to the meaning of our messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english') #loads the stop words for the english language\n",
    "sms_df.text = sms_df.text.apply(lambda x: \" \".join(x for x in x.split() if x not in stop)) \n",
    "#returns only words that are not in the list of stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correct Obvious Spelling issues:\n",
    "\n",
    "First, lets fix the most commonly 'missspelled' words. After, we will use textBlob to correct our spelling for all the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u         1132\n",
       "call       578\n",
       "2          482\n",
       "im         464\n",
       "ur         390\n",
       "get        386\n",
       "4          293\n",
       "dont       287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "got        239\n",
       "ill        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "text       188\n",
       "one        171\n",
       "going      171\n",
       "ü          169\n",
       "need       167\n",
       "txt        163\n",
       "home       162\n",
       "lor        160\n",
       "see        157\n",
       "sorry      156\n",
       "still      154\n",
       "r          153\n",
       "stop       152\n",
       "back       152\n",
       "n          146\n",
       "reply      144\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "later      134\n",
       "well       134\n",
       "hi         133\n",
       "think      132\n",
       "da         131\n",
       "please     129\n",
       "take       126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good amount of these are abbreviations, so lets change these in our dataset before we remove frequent and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = {' wif u ':' with you ', ' u r ':' you are ', 'ic': ' i see ', ' i c ': ' i see ',\n",
    "       ' u c ': ' you see ',' u ':' you ',' n ':' and ',' r ':' are ',' txt ':' text ',\n",
    "       ' ü ':' you ',' 4 ':' for ',' 2 ':' to ',' ur ':' your ',' da ':' the ',' wif ':' with ',\n",
    "       ' urself ':' yourself ', ' thats ': ' thats ', ' i‘m ': ' im ', '£':''}\n",
    "\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "sms_df.text = sms_df.text.apply(lambda x: pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip())\n",
    "# text = pattern.sub(lambda m: rep[m.group(0)], x.center(len(x)+2)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now if we look at the most common words, we can see the changes that we have made__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "you       1203\n",
       "see        961\n",
       "i          794\n",
       "call       578\n",
       "im         469\n",
       "to         464\n",
       "get        386\n",
       "your       349\n",
       "text       346\n",
       "e          320\n",
       "for        287\n",
       "dont       287\n",
       "go         281\n",
       "ok         278\n",
       "ltgt       276\n",
       "free       275\n",
       "know       257\n",
       "like       244\n",
       "ill        239\n",
       "got        239\n",
       "good       236\n",
       "come       229\n",
       "time       208\n",
       "k          207\n",
       "day        203\n",
       "love       200\n",
       "want       193\n",
       "send       191\n",
       "one        171\n",
       "going      171\n",
       "need       167\n",
       "home       162\n",
       "lor        160\n",
       "sorry      157\n",
       "still      154\n",
       "back       152\n",
       "stop       152\n",
       "are        149\n",
       "p          146\n",
       "reply      144\n",
       "and        141\n",
       "today      141\n",
       "mobile     138\n",
       "tell       137\n",
       "new        136\n",
       "later      134\n",
       "well       134\n",
       "hi         133\n",
       "think      132\n",
       "please     129\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:50]\n",
    "common_w "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Remove Very Frequent Words:\n",
    "Super common words do not add value to their connection with other words, which is why we will remove the top ten most common words in the cell below. Becuase this is text data, I decided it would be fine in this case to remove these words. [Here](https://www.quora.com/Why-do-we-remove-frequent-and-infrequent-words-when-in-NLP) is a great explaination for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most common word list:\n",
    "common_w = pd.Series(' '.join(sms_df.text).split()).value_counts()[:15]\n",
    "#Pretty good sign that most of these seem like filler words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now that we've checked on them, lets remove them from our texts\n",
    "sms_df.text = sms_df.text.apply(lambda x: ' '.join(x for x in x.split() if x not in list(common_w.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Use TextBlob to correct spelling: \n",
    "\n",
    "Now that we've removed a good portion of words that add noise to our data, we should clean up the words in one final step to correct spelling. \n",
    "\n",
    "There are many libraries to do this, but for convenience sake I have stuck with this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.to_csv('Data/spelling_and_features_sms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing:\n",
    "\n",
    "Great article [here](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python) about lemmatizing and stemming with NLTK.\n",
    "\n",
    "If you get an error about POS (part of speech) tagging, then you may need to download treebank and punkt from NLTK. [Here](https://stackoverflow.com/questions/14089887/nltk-pos-tag-usage) is the resource I used. However, mine did not work until I downloaded 'averaged_perceptron_tagger'.\n",
    "\n",
    "Below I defined a function that takes the returned pos values from nltk.pos_tag, and returns a compatible version of them. This was a great solution on [stackoverflow](https://stackoverflow.com/questions/5364493/lemmatizing-pos-tagged-words-with-nltk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = {\n",
    "        'CC':'', # coordin. conjunction (and, but, or)  \n",
    "        'CD':wn.NOUN, # cardinal number (one, two)             \n",
    "        'DT':'', # determiner (a, the)                    \n",
    "        'EX':wn.ADV, # existential ‘there’ (there)           \n",
    "        'FW':'', # foreign word (mea culpa)             \n",
    "        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   \n",
    "        'JJ': wn.ADJ, # adjective (yellow)                  \n",
    "        'JJR': wn.ADJ_SAT, # adj., comparative (bigger)          \n",
    "        'JJS': wn.ADJ_SAT, # adj., superlative (wildest)           \n",
    "        'LS':'', # list item marker (1, 2, One)          \n",
    "        'MD':'', # modal (can, should)                    \n",
    "        'NN':wn.NOUN, # noun, sing. or mass (llama)          \n",
    "        'NNS':wn.NOUN, # noun, plural (llamas)                  \n",
    "        'NNP':wn.NOUN, # proper noun, sing. (IBM)              \n",
    "        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)\n",
    "        'PDT':wn.ADJ, # predeterminer (all, both)            \n",
    "        'POS':'', # possessive ending (’s )               \n",
    "        'PRP':'', # personal pronoun (I, you, he)     \n",
    "        'PRP$':'', # possessive pronoun (your, one’s)    \n",
    "        'RB':wn.ADV, # adverb (quickly, never)            \n",
    "        'RBR':wn.ADV, # adverb, comparative (faster)        \n",
    "        'RBS':wn.ADV, # adverb, superlative (fastest)     \n",
    "        'RP':wn.ADJ, # particle (up, off)\n",
    "        'TO':'', # “to” (to)\n",
    "        'UH':'', # interjection (ah, oops)\n",
    "        'VB':wn.VERB, # verb base form (eat)\n",
    "        'VBD':wn.VERB, # verb past tense (ate)\n",
    "        'VBG':wn.VERB, # verb gerund (eating)\n",
    "        'VBN':wn.VERB, # verb past participle (eaten)\n",
    "        'VBP':wn.VERB, # verb non-3sg pres (eat)\n",
    "        'VBZ':wn.VERB, # verb 3sg pres (eats)\n",
    "        'WDT':'', # wh-determiner (which, that)\n",
    "        'WP':'', # wh-pronoun (what, who)\n",
    "        'WP$':'', # possessive (wh- whose)\n",
    "        'WRB':'' # wh-adverb (how, where)\n",
    "    }\n",
    "\n",
    "def get_wordnet_pos(treebank_tag,tag_map):\n",
    "    for word, initial in tag_map.items():\n",
    "        treebank_tag = treebank_tag.replace(word.lower(), initial)\n",
    "    if treebank_tag != '':\n",
    "        return treebank_tag\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemm_it(msg):\n",
    "    txt = []\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    tokens = word_tokenize(msg)\n",
    "    pos_tokens = nltk.pos_tag(tokens)\n",
    "    for word in pos_tokens:\n",
    "        txt.append(lemmatizer.lemmatize(word[0], pos=get_wordnet_pos(word[1])))\n",
    "    return txt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'VB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-05810b4d2403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mposes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/elenasm7/anaconda/lib/python3.6/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elenasm7/anaconda/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0;31m#    find a match or you can't go any further\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0mexceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m         \u001b[0msubstitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPHOLOGICAL_SUBSTITUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'VB'"
     ]
    }
   ],
   "source": [
    "msg = 'hey how are you doing today you look amazing'\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "tokens = word_tokenize(msg)\n",
    "poses = nltk.pos_tag(tokens)\n",
    "for i in poses: \n",
    "    print(lemmatizer.lemmatize(i[0], pos=get_wordnet_pos(i[1],tag_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-f37e0b9db76f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msms_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msms_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemm_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/elenasm7/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   2549\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-f37e0b9db76f>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(txt)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msms_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemmed'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msms_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlemm_it\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-66-e6cd419be2cc>\u001b[0m in \u001b[0;36mlemm_it\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_wordnet_pos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elenasm7/anaconda/lib/python3.6/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/elenasm7/anaconda/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36m_morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0;31m#    find a match or you can't go any further\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0mexceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m         \u001b[0msubstitutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMORPHOLOGICAL_SUBSTITUTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ''"
     ]
    }
   ],
   "source": [
    "sms_df['lemmed']=sms_df.text.apply(lambda txt: lemm_it(txt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       jurong point crazy available bugis and great w...\n",
       "1                                     lar joking with oni\n",
       "2       free entry wkly comp win fa cup final tkts 21s...\n",
       "3                           dun say early hor already say\n",
       "4                     nah think go usf life around though\n",
       "5       freemsg hey darling 3 week word back id like f...\n",
       "6           even brother like speak treat like aid patent\n",
       "7       per request melle melle oru minnaminunginte nu...\n",
       "8       winner valued network customer selected receiv...\n",
       "9       mobile 11 month are entitled update latest col...\n",
       "10      gonna home soon want talk stuff anymore tonigh...\n",
       "11      six chance win cash 100 20000 pound csh11 send...\n",
       "12      urgent 1 week free membership 100000 prize jac...\n",
       "13      ive searching right word thank breather promis...\n",
       "14                                            date sunday\n",
       "15      xxxmobilemovieclub use credit cl k wap link ne...\n",
       "16                                        oh kim watching\n",
       "17        eh remember spell name yes v naughty make v wet\n",
       "18                   fine thats way feel thats way gota b\n",
       "19      england v macedonia miss goalsteam news ur nat...\n",
       "20                                   seriously spell name\n",
       "21                           going try month ha ha joking\n",
       "22                          pay first lar the stock comin\n",
       "23      aft finish lunch str lor ard 3 smth lor finish...\n",
       "24                     ffffffffff alright way meet sooner\n",
       "25      forced eat sl really hungry tho suck mark gett...\n",
       "26                                  lol always convincing\n",
       "27      catch bus frying egg make tea eating mom left ...\n",
       "28           back amp packing car ill let know there room\n",
       "29               ahhh work vaguely remember feel like lol\n",
       "                              ...                        \n",
       "5544                                armand say as epsilon\n",
       "5545                  still havent got yourself jacket ah\n",
       "5546    taking derek amp taylor walmart back time your...\n",
       "5547                               hi durban still number\n",
       "5548                                  lotta childporn car\n",
       "5549    contract mobile 11 mnths latest motorola nokia...\n",
       "5550                                     trying weekend v\n",
       "5551    know wot people wear shirt jumper hat belt kno...\n",
       "5552                                      cool time think\n",
       "5553                       wen spiritual deep thats great\n",
       "5554    safe trip nigeria wish happiness soon company ...\n",
       "5555                                 hahahause brain dear\n",
       "5556    well keep mind ive got enough gas one round tr...\n",
       "5557    yeh indian n tho kane bit shud drink sometime ...\n",
       "5558                   yes thats texted pshewmissing much\n",
       "5559    meant calculation unit school really expensive...\n",
       "5560                                      sorry ill later\n",
       "5561                       arent next hour imma flip shit\n",
       "5562                               anything lor juz u lor\n",
       "5563              dump heap mom decided come lowes boring\n",
       "5564    lor sony er sson salesman ask shuhui say quite...\n",
       "5565                                   ard 6 like dat lor\n",
       "5566                             wait til least wednesday\n",
       "5567                                              huh lei\n",
       "5568    reminder o2 250 pound free credit detail great...\n",
       "5569    2nd time tried contact u 750 pound prize claim...\n",
       "5570                            b going esplanade fr home\n",
       "5571                           pity mood soany suggestion\n",
       "5572    guy bitching acted like id interested buying s...\n",
       "5573                                       rofl true name\n",
       "Name: lemmed, Length: 5574, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_df.lemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the words in each message\n",
    "\n",
    "below we will use the nltk word tokenizer to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_message(row):\n",
    "    return word_tokenize(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.text = sms_df.text.apply(lambda row: tokenize_message(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_words(row,li):\n",
    "    li.append(' '.join(set(row)))\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_of_words = []\n",
    "sms_df.text.apply(lambda row: get_all_words(row,list_of_words));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list_of_words = ' '.join(list_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_list_of_words = new_list_of_words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(new_list_of_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
